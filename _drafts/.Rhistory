doc = pptx( title = "title" )
names(doc)
doc[1]
doc[2]
dot[3]
doc[3]
doc[4]
slide.layouts(doc)
doc = addSlide( doc, slide.layout = "Title Slide" )
doc = addTitle( doc, "Presentation title" ) #set the main title
doc = addSubtitle( doc , "This document is generated with ReporteRs.")#set the sub-title
doc = addSlide( doc, slide.layout = "Two Content" )
doc = addTitle( doc, "Texts demo" ) #set the main title
texts = c( "Lorem ipsum dolor sit amet, consectetur adipiscing elit."
, "In sit amet ipsum tellus. Vivamus dignissim arcu sit amet faucibus auctor."
, "Quisque dictum tristique ligula."
)
# add simple text
doc = addParagraph( doc, value = texts)
library(XML)
library(RCurl)
Q =
Q
?format
QRY = sprintf("http://www.bing.com/search?q=undergrad+admission+twitter&first=%i", 0)
QRY
page = getURL(QRY)
doc = htmlParse(page)
links = unlist(doc['//@href'])
names(links) <- NULL
View(links)
library(stringr)
twitter_links = links[str_detect(links, "twitter.com")]
twitter_liks = unique(twitter_links)
twitter_links = unique(twitter_links)
twitter_links
library(d3Network)
Source <- c("A", "A", "A", "A", "B", "B", "C", "C", "D")
Target <- c("B", "C", "D", "J", "E", "F", "G", "H", "I")
NetworkData <- data.frame(Source, Target)
d3SimpleNetwork(NetworkData, width = 400, height = 250)
d3ForceNetwork
DDIR = "~/Dropbox/Analytics/DW/data"
list.files(DDIR)
apps = readRDS(file.path(DDIR, "uadm.rds"))
cont = readRDS(file.path(DDIR, "cont.rds"))
apps = subset(apps, term = '201409')
apps = subset(apps, term == '201409')
head(cont)
cont_a = subset(cont, pidm %in% apps$pidm)
head(cont_a)
devtools::install_github("rstudio/rticles")
install.packages("installr")
install.packages("installr")
install.packages("devtools")
devtools::install_github("klutometis/roxygen")
library(roxygen2)
?create
library(devtools)
?create
create("contactR")
library(roxygen2)
library(streamR)
system("python /Users/brock/Desktop/test-rstudio-python.py")
system("python /Users/brock/Desktop/test-rstudio-python.py")
install.packages("SocialMediaMineR")
library(SocialMediaMineR)
tmp = get_facebook("http://www.bentley.edu/undergraduate")
tmp
get_facebook
###############################################################################
## Roll up the performance for search emails
###############################################################################
## load the packages
library(stringr)
library(plyr)
## bring in the datasets
DDIR = "~/Dropbox/Analytics/DW/data"
comm = readRDS(file.path(DDIR, "comm-raw.rds"))
open = readRDS(file.path(DDIR, "open-raw.rds"))
## keep only search emails
search = subset(comm, str_detect(tolower(refdesc), "search"))
search = subset(comm, str_detect(refdesc, "OS_MKT_201509_Search"))
search = subset(search, !str_detect(refdesc, "Seed"))
tmp = ddply(search, .(refdesc), summarise, num = length(pidm))
tmp = subset(tmp, num > 100)
search = subset(search, refdesc %in% tmp$refdesc)
rm(tmp)
## keep only the summary data that makes sense
links = subset(open, msgid %in% search$msgid)
## use plyr to roll up the stats
stats = ddply(links, .(msgid), summarise,
open = 1,
click = max(ifelse(type == 'LINK', 1, 0)))
## add back on to the search data -- left join to retain non-engagement
search_perf = merge(search, stats, all.x=T)
search_perf$open[is.na(search_perf$open)] = 0
tmp = search_perf[1:5, ]
tmp
transform(tmp, wave = str_extract(refdesc, 'W[1-9]{1}'))
transform(tmp, wave = str_extract(refdesc, 'W[1-9]{1}'), email = str_extract(refdesc, 'E[1-10]'))
search_perf = transform(search_perf,
wave = str_extract(refdesc, 'W[1-9]{1}'),
email = str_extract(refdesc, 'E[1-10]'))
head(search_perf)
with(search_perf, table(wave))
with(search_perf, table(email))
tail(search_perf)
search_perf = transform(search_perf,
wave = str_extract(refdesc, 'W[1-9]{1}'),
email = str_extract(refdesc, 'E[0-9]{1,2}'))
with(search_perf, table(wave))
with(search_perf, table(email))
ls()
perf
head(search_perf)
perf = ddply(search_perf, (wave, email), mutate,
deliv = sum(delivflag),
open = sum(open),
click = sum(click, na.rm=T),
open_rate = open / deliv,
click_rate = click / open)
perf = ddply(search_perf, (wave, email), summarise,
deliv = sum(delivflag),
open = sum(open),
click = sum(click, na.rm=T),
open_rate = sum(open) / sum(delivflag),
click_rate = sum(click, na.rm=T) / sum(open))
perf = ddply(search_perf, .(wave, email), summarise,
deliv = sum(delivflag),
open = sum(open),
click = sum(click, na.rm=T),
open_rate = sum(open) / sum(delivflag),
click_rate = sum(click, na.rm=T) / sum(open))
str(search_perf)
perf = ddply(search_perf, .(wave, email), summarise,
deliv = sum(as.numeric(delivflag)),
open = sum(open),
click = sum(click, na.rm=T),
open_rate = sum(open) / sum(as.numeric(delivflag)),
click_rate = sum(click, na.rm=T) / sum(open))
head(perf)
perf = transform(perf, email = gsub("E", "", email))
head(perf)
perf = arrange(perf, wave, email)
View(perf)
head(perf)
perf = ddply(search_perf, .(wave, email), summarise,
deliv = sum(as.numeric(delivflag)),
open = sum(open),
click = sum(click, na.rm=T),
open_rate = sum(open) / sum(as.numeric(delivflag)),
click_rate = sum(click, na.rm=T) / sum(open))
perf = transform(perf, email = as.numeric(gsub("E", "", email)))
perf = arrange(perf, wave, email)
head(perf)
View(perf)
perf
sum(perf$click)
library(RNeo4j)
?clear
graph = startGraph("http://10.0.0.3:7474/db/data/")
library(RCurl)
library(rjson)
URL = "https://raw.githubusercontent.com/christophergandrud/networkD3/master/JSONdata/energy.json"
raw_dat = getURL(URL)
raw_j = toJSON(raw_dat)
names(raw_j)
length(raw_j)
raw_j
devtools::install_github("christophergandrud/networkD3")
library(networkD3)
library(networkD3)
EngLinks <- JSONtoDF(jsonStr = Energy, array = "links")
URL <- "https://raw.githubusercontent.com/christophergandrud/d3Network/sankey/JSONdata/energy.json"
Energy <- getURL(URL, ssl.verifypeer = FALSE)
# Convert to data frame
EngLinks <- JSONtoDF(jsonStr = Energy, array = "links")
EngNodes <- JSONtoDF(jsonStr = Energy, array = "nodes")
raw_j = fromJSON(raw_dat)
names(raw_j)
links <- JSONtoDF(jsonStr = Energy, array = "links")
nodes <- JSONtoDF(jsonStr = Energy, array = "nodes")
head(links)
head(nodes)
d3Sankey(Links = EngLinks, Nodes = EngNodes, Source = "source",
Target = "target", Value = "value", NodeID = "name",
fontsize = 12, nodeWidth = 30, width = 700)
library(networkD3)
d3Sankey(Links = EngLinks, Nodes = EngNodes, Source = "source",
Target = "target", Value = "value", NodeID = "name",
fontsize = 12, nodeWidth = 30, width = 700)
nodes <- JSONtoDF(jsonStr = Energy, array = "nodes")
sankeyNetwork(Links = EngLinks, Nodes = EngNodes, Source = "source",
Target = "target", Value = "value", NodeID = "name",
fontsize = 12, nodeWidth = 30, width = 700)
?JSONtoDF
devtools::install_github("btibert3/prismaticR")
library(prismaticR)
TOKEN = "MTQyMzQwOTM4MzUxOA.cHJvZA.YnRpYmVydDNAZ21haWwuY29t.Vz2n7Zd1ks8UZ4j9fDpU3MWVogk"
prizURL(TOKEN, "https://en.wikipedia.org/wiki/New_England_Patriots")
prizURL
setwd("~/github/prismaticR")
devtools::install_github("btibert3/prismaticR")
library(prismaticR)
TOKEN = "MTQyMzQwOTM4MzUxOA.cHJvZA.YnRpYmVydDNAZ21haWwuY29t.Vz2n7Zd1ks8UZ4j9fDpU3MWVogk"
prizURL(TOKEN, "https://en.wikipedia.org/wiki/New_England_Patriots")
prizURL
detach("package:prismaticR", unload=TRUE)
devtools::install_github("btibert3/prismaticR")
library(prismaticR)
prizURL(TOKEN, "https://en.wikipedia.org/wiki/New_England_Patriots")
detach("package:prismaticR", unload=TRUE)
devtools::install_github("btibert3/prismaticR")
library(prismaticR)
prizURL(TOKEN, "https://en.wikipedia.org/wiki/New_England_Patriots")
prizSIM(TOKEN, 1950)
prizURL(TOKEN, "https://en.wikipedia.org/wiki/New_England_Patriots")
prizSIM(TOKEN, 1950)
detach("package:prismaticR", unload=TRUE)
devtools::install_github("btibert3/prismaticR")
library(prismaticR)
detach("package:prismaticR", unload=TRUE)
devtools::document()
devtools::install_github("btibert3/prismaticR")
library(prismaticR)
library(prismaticR)
?prizURL
TOKEN = "MTQyMzQwOTM4MzUxOA.cHJvZA.YnRpYmVydDNAZ21haWwuY29t.Vz2n7Zd1ks8UZ4j9fDpU3MWVogk"
URL = "http://www.bentley.edu/about/mission-vision-and-values"
EP = "http://interest-graph.getprismatic.com/url/aspect"
if (is.na(TOKEN)) {
message("You must supply a valid token")
}
if (is.na(URL)) {
message("You must supply a valid URL in the form of http://....")
}
PAGE = paste(EP,
"?api-token=",
TOKEN,
"&url=",
RCurl::curlEscape(URL),
sep = "")
PAGE
resp = httr::GET(PAGE)
resp$status_code
length(content(resp)[[1]])
suppressMessages(library(httr))
length(content(resp)[[1]])
dat = httr::content(resp)
names(dat)
names(dat$aspects)
dat$aspects$type
URL = "http://www.barstoolsports.com/boston/akron-ohio-is-on-the-hunt-for-a-diabolical-poo-bandit-who-keeps-shitting-on-everyons-car/"
x = prizSIM(TOKEN, URL)
source('~/github/prismaticR/R/prizASP.R', echo=TRUE)
x = prizASP(TOKEN, URL)
x
x
URL
URL = "https://medium.com/the-year-of-the-looking-glass/the-idea-person-e08e36f9024d"
x = prizASP(TOKEN, URL)
x
x
URL = "https://twitter.com/UGABentley/status/575770847899578368"
x = prizASP(TOKEN, URL)
x
URL = "bit.ly/1NLUNVj"
x = prizASP(TOKEN, URL)
x
URL = "https://www.easports.com/nhl/news/2015/nhl-15-hockey-ultimate-team-team-of-the-week"
x = prizASP(TOKEN, URL)
x
URL = "http://www.hockey-reference.com"
x = prizASP(TOKEN, URL)
x
URL = "https://en.wikipedia.org/wiki/New_England_Patriots"
x = prizASP(TOKEN, URL)
x
getwd()
document()
library(devtools)
library(roxygen2)
document()
library(httr)
library(rvest)
URL = "http://interest-graph.getprismatic.com/topic/all/human"
resp = html(URL)
names(resp)
resp
mode(resp)
tids = html_table(resp)
tids = html_table(resp, "table")
tids = html_table(resp, header=F)
tids[1]
tids[[1]]
tables = html_nodes(resp, "table")
tids = html_table[tables[[1]]]
tids = html_table(tables[[1]])
head(tids)
document()
devtools::document()
setwd("..")
setwd("prismaticR/")
list.files()
library(httr)
TOKEN = "MTQyMzQwOTM4MzUxOA.cHJvZA.YnRpYmVydDNAZ21haWwuY29t.Vz2n7Zd1ks8UZ4j9fDpU3MWVogk"
EP = "http://interest-graph.getprismatic.com/topic/recent-url"
TID = 1950
PAGE = paste(EP,
"?api-token=",
TOKEN,
"&id=",
RCurl::curlEscape(TID),
sep = "")
PAGE
resp = httr::GET(PAGE)
if (resp$status_code != 200) {
df = data.frame()
} else if (length(content(resp)[[1]]) == 0) {
## checks if no topics are returned by the API
df = data.frame()
} else {
## parse the data into a nested list
dat = httr::content(resp)
## parse into a dataframe
df = do.call(rbind, lapply(dat$topics, function(x) data.frame(topic_id = x$id,
topic = x$topic,
score = x$score,
stringsAsFactors=F)))
}
PAGE = paste(EP,
"?api-token=",
TOKEN,
"&id=",
RCurl::curlEscape(TID),
sep = "")
## get the data
resp = httr::GET(PAGE)
if (resp$status_code != 200) {
df = data.frame()
} else if (length(content(resp)[[1]]) == 0) {
## checks if no topics are returned by the API
df = data.frame()
} else {
## parse the data into a nested list
dat = httr::content(resp)
## parse into a dataframe
df = do.call(rbind, lapply(dat$topics, function(x) data.frame(topic_id = x$id,
topic = x$topic,
score = x$score,
stringsAsFactors=F)))
resp = httr::GET(PAGE)
if (resp$status_code != 200) {
df = data.frame()
} else if (length(content(resp)[[1]]) == 0) {
## checks if no topics are returned by the API
df = data.frame()
} else {
## parse the data into a nested list
dat = httr::content(resp)
## parse into a dataframe
df = do.call(rbind, lapply(dat$topics, function(x) data.frame(topic_id = x$id,
topic = x$topic,
score = x$score,
stringsAsFactors=F)))
}
content(resp)
resp = httr::GET(PAGE)
if (resp$status_code != 200) {
df = data.frame()
} else if (length(content(resp)[[1]]) == 0) {
## checks if no topics are returned by the API
df = data.frame()
} else {
## parse the data into a nested list
dat = httr::content(resp)
## parse into a dataframe
df = do.call(rbind, lapply(dat$topics, function(x) data.frame(topic_id = TID,
url = x$url,
score = x$score,
stringsAsFactors=F)))
}
dat = httr::content(resp)
dat
df = do.call(rbind, lapply(dat$topics, function(x) data.frame(topic_id = TID,
url = x$url,
score = x$score,
stringsAsFactors=F)))
df = do.call(rbind, lapply(dat$urls, function(x) data.frame(topic_id = TID,
url = x$url,
score = x$score,
stringsAsFactors=F)))
PAGE = paste(EP,
"?api-token=",
TOKEN,
"&id=",
RCurl::curlEscape(TID),
sep = "")
## get the data
resp = httr::GET(PAGE)
if (resp$status_code != 200) {
df = data.frame()
} else if (length(content(resp)[[1]]) == 0) {
## checks if no topics are returned by the API
df = data.frame()
} else {
## parse the data into a nested list
dat = httr::content(resp)
## parse into a dataframe
df = do.call(rbind, lapply(dat$urls, function(x) data.frame(topic_id = TID,
url = x$url,
score = x$score,
stringsAsFactors=F)))
}
View(df)
devtools::document()
devtools::install_github("btibert3/prismaticR")
library(prismaticR)
x = prizTID()
head(x)
subset(x, id==1950)
TOKEN = "MTQyMzQwOTM4MzUxOA.cHJvZA.YnRpYmVydDNAZ21haWwuY29t.Vz2n7Zd1ks8UZ4j9fDpU3MWVogk"
x = prizRECENT(TOKEN, 1950)
x
View(x)
library(stringr)
tids = prizTID()
head(tids)
tids$topic = tolower(tids$topic)
head(topic)
head(tids)
tids$topic = tolower(tids$topic)
head(tids)
str_detect(tids$topic, "admission")
tids[str_detect(tids$topic, "admission"),]
tids[str_detect(tids$topic, "enrollment"),]
tids[str_detect(tids$topic, "higher ed"),]
tids[str_detect(tids$topic, "financial aid"),]
tids[str_detect(tids$topic, "college"),]
prizSIM(TOKEN, 993)
prizSIM(TOKEN, '993')
prizSIM(TOKEN, '996')
prizSIM(TOKEN, '182')
tids[str_detect(tids$topic, "student"),]
prizSIM(TOKEN, 4249)
tids[str_detect(tids$topic, "university"),]
setwd("~/github/btibert3.github.io/_drafts")
library(RNeo4j)
library(devtools)
install_github("btibert3/prismaticR")
library(prismaticR)
library(prismaticR)
Sys.getenv()
Sys.getenv("PRIZ_TOKEN")
TOKEN
TOKEN = "MTQyMzQwOTM4MzUxOA.cHJvZA.YnRpYmVydDNAZ21haWwuY29t.Vz2n7Zd1ks8UZ4j9fDpU3MWVogk"
Sys.getenv("PRIZ_TOKEN")
Sys.getenv("PRIZ_TOKEN")
TOKEN = "MTQyMzQwOTM4MzUxOA.cHJvZA.YnRpYmVydDNAZ21haWwuY29t.Vz2n7Zd1ks8UZ4j9fDpU3MWVogk"
TOKEN = "MTQyMzQwOTM4MzUxOA.cHJvZA.YnRpYmVydDNAZ21haWwuY29t.Vz2n7Zd1ks8UZ4j9fDpU3MWVogk"
tids = prizTID()
tids$topic = tolower(tids$topic)
tids[str_detect(tids$topic, "admission"),]
library(stringr)
tids[str_detect(tids$topic, "admission"),]
tids[str_detect(tids$topic, "enrollment"),]
tids[str_detect(tids$topic, "higher ed"),]
tids[str_detect(tids$topic, "college"),]
tids[str_detect(tids$topic, "university"),]
harvard = tids[str_detect(tids$topic, "harvard uni")]
harvard = tids[str_detect(tids$topic, "harvard uni"),]
harvard
harvard = tids[str_detect(tids$topic, "harvard uni"),]$id
harvard
prizSIM(TOKEN, TID = harvard)
prizSIM(TOKEN, '996')
prizSIM(TOKEN, 993)
prizSIM(TOKEN, 4249)
prizSIM(TOKEN, '182')
tids[str_detect(tids$topic, "admission"),]
tids[str_detect(tids$topic, "enrollment"),]
tids[str_detect(tids$topic, "higher ed"),]
tids[str_detect(tids$topic, "financial aid"),]
tids[str_detect(tids$topic, "college"),]
prizSIM(TOKEN, 993)
prizSIM(TOKEN, '996')
prizSIM(TOKEN, '182')
tids[str_detect(tids$topic, "student"),]
prizSIM(TOKEN, 4249)
prizSIM(TOKEN, '182')
prizSIM(TOKEN, TID = 182)
tids[str_detect(tids$topic, "admission"),]
prizRECENT(TOKEN, 993)
x = prizRECENT(TOKEN, 993)
x = arrange(x, desc(score))
x = arrange(x, desc(score))
library(dplyr)
x = arrange(x, desc(score))
head(x)
page
x_resp = html(x)
x
x_resp = html(x$url[1])
html_node(x_resp, "title")
html_node(x_resp, "title") %>% html_text()
cd ..
ls
list.files()
library(prismaticR)
library(prismaticR)
tids = prizTID()
